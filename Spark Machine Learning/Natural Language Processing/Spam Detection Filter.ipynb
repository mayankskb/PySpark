{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('spam classifier').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv('smsspamcollection/SMSSpamCollection', inferSchema=True,\n",
    "                     sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|             message|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "| spam|England v Macedon...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed('_c0','class').withColumnRenamed('_c1','message')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and Prepare the Data\n",
    "** Create a new length feature: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------+\n",
      "|class|             message|length|\n",
      "+-----+--------------------+------+\n",
      "|  ham|Go until jurong p...|   111|\n",
      "|  ham|Ok lar... Joking ...|    29|\n",
      "| spam|Free entry in 2 a...|   155|\n",
      "|  ham|U dun say so earl...|    49|\n",
      "|  ham|Nah I don't think...|    61|\n",
      "| spam|FreeMsg Hey there...|   147|\n",
      "|  ham|Even my brother i...|    77|\n",
      "|  ham|As per your reque...|   160|\n",
      "| spam|WINNER!! As a val...|   157|\n",
      "| spam|Had your mobile 1...|   154|\n",
      "|  ham|I'm gonna be home...|   109|\n",
      "| spam|SIX chances to wi...|   136|\n",
      "| spam|URGENT! You have ...|   155|\n",
      "|  ham|I've been searchi...|   196|\n",
      "|  ham|I HAVE A DATE ON ...|    35|\n",
      "| spam|XXXMobileMovieClu...|   149|\n",
      "|  ham|Oh k...i'm watchi...|    26|\n",
      "|  ham|Eh u remember how...|    81|\n",
      "|  ham|Fine if thats th...|    56|\n",
      "| spam|England v Macedon...|   155|\n",
      "+-----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumn('length', length(data['message']))\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets get some insights for the difference in the length for the messages of these two class messages that asre spam and ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|class|      avg(length)|\n",
      "+-----+-----------------+\n",
      "|  ham|71.45431945307645|\n",
      "| spam|138.6706827309237|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupBy('class').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the idea about the average difference in length of the messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets do some actual text Processing for the data\n",
    "\n",
    "**Preparing the data**\n",
    "\n",
    "<b>Feature Transformation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (Tokenizer, CountVectorizer, \n",
    "                                StopWordsRemover, IDF, StringIndexer)\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'message', outputCol = 'token')\n",
    "stop_words = StopWordsRemover(inputCol='token', outputCol='stoptoken')\n",
    "c_vec = CountVectorizer(inputCol='stoptoken', outputCol = 'count_vec')\n",
    "idf = IDF(inputCol='count_vec', outputCol = 'tf_idf')\n",
    "ham_spam_label = StringIndexer(inputCol='class', outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = ['tf_idf','length'],\n",
    "    outputCol = 'features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#lets get a pipeline and lined up our feature transformation steps so that they will be carried out in a sequence vis a pipeline object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_pipelines = Pipeline(stages = [ham_spam_label,tokenizer, stop_words, c_vec, idf, assembler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleandata = data_pipelines.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- message: string (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- stoptoken: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- count_vec: vector (nullable = true)\n",
      " |-- tf_idf: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleandata.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(13424,[7,11,31,6...|  0.0|\n",
      "|(13424,[0,24,297,...|  0.0|\n",
      "|(13424,[2,13,19,3...|  1.0|\n",
      "|(13424,[0,70,80,1...|  0.0|\n",
      "|(13424,[36,134,31...|  0.0|\n",
      "|(13424,[10,60,139...|  1.0|\n",
      "|(13424,[10,53,103...|  0.0|\n",
      "|(13424,[125,184,4...|  0.0|\n",
      "|(13424,[1,47,118,...|  1.0|\n",
      "|(13424,[0,1,13,27...|  1.0|\n",
      "|(13424,[18,43,120...|  0.0|\n",
      "|(13424,[8,17,37,8...|  1.0|\n",
      "|(13424,[13,30,47,...|  1.0|\n",
      "|(13424,[39,96,217...|  0.0|\n",
      "|(13424,[552,1697,...|  0.0|\n",
      "|(13424,[30,109,11...|  1.0|\n",
      "|(13424,[82,214,47...|  0.0|\n",
      "|(13424,[0,2,49,13...|  0.0|\n",
      "|(13424,[0,74,105,...|  0.0|\n",
      "|(13424,[4,30,33,5...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mldata = cleandata.select('features','label')\n",
    "mldata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making train-test splits\n",
    "train, test = mldata.randomSplit([0.8,0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lets get our model\n",
    "classifier = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#deploy it on test data\n",
    "test_results = classifier.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(13424,[0,1,2,20,...|  1.0|[-1163.0881608848...|[2.37506684030818...|       1.0|\n",
      "|(13424,[0,1,4,13,...|  1.0|[-1402.6063348349...|[1.49150553040185...|       1.0|\n",
      "|(13424,[0,1,7,15,...|  0.0|[-658.95819652588...|[1.0,7.3168063391...|       0.0|\n",
      "|(13424,[0,1,11,32...|  0.0|[-886.31676528524...|[1.0,8.8798391632...|       0.0|\n",
      "|(13424,[0,1,13,18...|  1.0|[-1170.3386704834...|[3.14961771197959...|       1.0|\n",
      "|(13424,[0,1,14,31...|  0.0|[-215.27949490282...|[1.0,1.5083918451...|       0.0|\n",
      "|(13424,[0,1,47,59...|  1.0|[-1376.9629711573...|[1.47918499536610...|       1.0|\n",
      "|(13424,[0,1,874,1...|  0.0|[-95.036910642319...|[0.99999999373945...|       0.0|\n",
      "|(13424,[0,1,3657,...|  0.0|[-128.73642119737...|[0.99996715597262...|       0.0|\n",
      "|(13424,[0,2,3,4,6...|  0.0|[-1278.2191561489...|[1.0,1.3238062612...|       0.0|\n",
      "|(13424,[0,2,4,8,1...|  0.0|[-1315.0094094080...|[1.0,1.8821653052...|       0.0|\n",
      "|(13424,[0,2,4,8,2...|  0.0|[-549.92028248108...|[1.0,1.2522017206...|       0.0|\n",
      "|(13424,[0,2,4,10,...|  0.0|[-1237.4515855991...|[1.0,3.6077030571...|       0.0|\n",
      "|(13424,[0,2,4,19,...|  1.0|[-1494.2819972426...|[1.78923899202332...|       1.0|\n",
      "|(13424,[0,2,7,8,1...|  0.0|[-704.75647618976...|[1.0,5.1883717834...|       0.0|\n",
      "|(13424,[0,2,7,11,...|  0.0|[-758.68827313304...|[1.0,6.0317650284...|       0.0|\n",
      "|(13424,[0,2,7,11,...|  0.0|[-1351.3485833379...|[1.0,1.2438359834...|       0.0|\n",
      "|(13424,[0,2,7,27,...|  0.0|[-473.11193033002...|[0.00167345997986...|       1.0|\n",
      "|(13424,[0,2,8,12,...|  0.0|[-1321.4220905337...|[1.0,1.3764207594...|       0.0|\n",
      "|(13424,[0,2,10,13...|  0.0|[-4841.5168353017...|           [1.0,0.0]|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import (BinaryClassificationEvaluator, MulticlassClassificationEvaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The area under ROC curve for Naive Bayes model for test dataset : 0.940118028234205\n"
     ]
    }
   ],
   "source": [
    "bineval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='label')\n",
    "area = bineval.evaluate(test_results)\n",
    "print(\"The area under ROC curve for Naive Bayes model for test dataset : {}\".format(area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this result is pretty good.\n",
    "Now lets get some accuracy metric for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 92.07%\n"
     ]
    }
   ],
   "source": [
    "acc = MulticlassClassificationEvaluator(metricName='accuracy')\n",
    "accuracy = acc.evaluate(test_results)\n",
    "\n",
    "print(\"Accuracy : {0:0.2f}%\".format(accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
